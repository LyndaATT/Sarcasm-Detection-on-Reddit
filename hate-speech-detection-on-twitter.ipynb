{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"!pip install openpyxl\n!pip install PyArabic\n!pip install git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git &> /dev/null\n!pip install emoji \n!pip install Arabic-Stopwords\n!pip install tkseem\n!pip install tnkeeh","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-14T15:28:05.929377Z","iopub.execute_input":"2022-02-14T15:28:05.930164Z","iopub.status.idle":"2022-02-14T15:29:11.048686Z","shell.execute_reply.started":"2022-02-14T15:28:05.930115Z","shell.execute_reply":"2022-02-14T15:29:11.047756Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"!pip3 install fr-word-segment","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:11.051898Z","iopub.execute_input":"2022-02-14T15:29:11.052246Z","iopub.status.idle":"2022-02-14T15:29:20.065136Z","shell.execute_reply.started":"2022-02-14T15:29:11.052201Z","shell.execute_reply":"2022-02-14T15:29:20.064097Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"!pip install pyspellchecker","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:20.067162Z","iopub.execute_input":"2022-02-14T15:29:20.067486Z","iopub.status.idle":"2022-02-14T15:29:29.094129Z","shell.execute_reply.started":"2022-02-14T15:29:20.067451Z","shell.execute_reply":"2022-02-14T15:29:29.093188Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\n\nimport nltk\nimport string\nfrom french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\nfrom fastai.text.all import *\n\nimport sklearn\nimport regex as re\nfrom unicodedata import normalize\n\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import AdamW\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import ToTensor\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import CyclicLR\nfrom torchvision import models\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport os\nimport gensim\n\n\n# keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import GRU,MaxPooling1D,GlobalMaxPooling1D,Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\nfrom keras import callbacks\n\n# sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer \nfrom sklearn.metrics import roc_auc_score, accuracy_score,roc_curve, auc, plot_confusion_matrix, confusion_matrix\nfrom sklearn.svm import LinearSVC\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.manifold import TSNE\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.preprocessing.text import Tokenizer\nimport emoji\nimport seaborn as sn\nimport pyarabic.araby as ar\nimport tkseem as tk\nimport tnkeeh as tn\nfrom nltk.stem.isri import ISRIStemmer\nfrom spellchecker import SpellChecker\nfrom wordsegment import load,segment\nload()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:29.097582Z","iopub.execute_input":"2022-02-14T15:29:29.098342Z","iopub.status.idle":"2022-02-14T15:29:29.919890Z","shell.execute_reply.started":"2022-02-14T15:29:29.098294Z","shell.execute_reply":"2022-02-14T15:29:29.918943Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"## Loading Data","metadata":{}},{"cell_type":"code","source":"df_ar = pd.read_csv('/kaggle/input/twitter/ar_dataset.csv')\ndf_ar2 = pd.read_excel('/kaggle/input/twitter/arr.xlsx')\n\n#main data\ndf_fr = pd.read_csv('/kaggle/input/twitter/fr_dataset.csv')\ndf_fr2 = pd.read_csv('/kaggle/input/twitter/french_tweets.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:29.921312Z","iopub.execute_input":"2022-02-14T15:29:29.921658Z","iopub.status.idle":"2022-02-14T15:29:33.069151Z","shell.execute_reply.started":"2022-02-14T15:29:29.921616Z","shell.execute_reply":"2022-02-14T15:29:33.068259Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"markdown","source":"## Exploring data","metadata":{}},{"cell_type":"markdown","source":"#### French Dataset","metadata":{}},{"cell_type":"markdown","source":"###### 1) Main dataset","metadata":{}},{"cell_type":"code","source":"df_fr.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.070409Z","iopub.execute_input":"2022-02-14T15:29:33.070641Z","iopub.status.idle":"2022-02-14T15:29:33.084370Z","shell.execute_reply.started":"2022-02-14T15:29:33.070612Z","shell.execute_reply":"2022-02-14T15:29:33.083606Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"df_fr.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.085699Z","iopub.execute_input":"2022-02-14T15:29:33.085980Z","iopub.status.idle":"2022-02-14T15:29:33.112933Z","shell.execute_reply.started":"2022-02-14T15:29:33.085949Z","shell.execute_reply":"2022-02-14T15:29:33.111707Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"print('Size of the dataset:')\nlen(df_fr)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.114199Z","iopub.execute_input":"2022-02-14T15:29:33.114984Z","iopub.status.idle":"2022-02-14T15:29:33.121896Z","shell.execute_reply.started":"2022-02-14T15:29:33.114938Z","shell.execute_reply":"2022-02-14T15:29:33.120972Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"#test if the data contains null values\nprint('Nan value',df_fr.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.123320Z","iopub.execute_input":"2022-02-14T15:29:33.123662Z","iopub.status.idle":"2022-02-14T15:29:33.137562Z","shell.execute_reply.started":"2022-02-14T15:29:33.123626Z","shell.execute_reply":"2022-02-14T15:29:33.136655Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"#take a look at the column of the dataframe to see the features\ndf_fr.columns","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.140514Z","iopub.execute_input":"2022-02-14T15:29:33.141337Z","iopub.status.idle":"2022-02-14T15:29:33.147165Z","shell.execute_reply.started":"2022-02-14T15:29:33.141303Z","shell.execute_reply":"2022-02-14T15:29:33.146416Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"df_fr['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.148526Z","iopub.execute_input":"2022-02-14T15:29:33.149046Z","iopub.status.idle":"2022-02-14T15:29:33.164401Z","shell.execute_reply.started":"2022-02-14T15:29:33.148998Z","shell.execute_reply":"2022-02-14T15:29:33.163617Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"#### Class distribution \ncum = df_fr['target'].value_counts().to_frame()\ncum['HITId'] = cum.index\ncumfig, ax = plt.subplots(figsize=(5,5))\nsn.barplot(data=cum,x='HITId',y='target',ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.166049Z","iopub.execute_input":"2022-02-14T15:29:33.166463Z","iopub.status.idle":"2022-02-14T15:29:33.384216Z","shell.execute_reply.started":"2022-02-14T15:29:33.166413Z","shell.execute_reply":"2022-02-14T15:29:33.383586Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"markdown","source":"###### 2) Assest dataset","metadata":{}},{"cell_type":"code","source":"df_fr2.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.385531Z","iopub.execute_input":"2022-02-14T15:29:33.385972Z","iopub.status.idle":"2022-02-14T15:29:33.395472Z","shell.execute_reply.started":"2022-02-14T15:29:33.385940Z","shell.execute_reply":"2022-02-14T15:29:33.394506Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"print('Size of the dataset:')\nlen(df_fr2)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.397418Z","iopub.execute_input":"2022-02-14T15:29:33.397936Z","iopub.status.idle":"2022-02-14T15:29:33.411398Z","shell.execute_reply.started":"2022-02-14T15:29:33.397891Z","shell.execute_reply":"2022-02-14T15:29:33.410339Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"#test if the data contains null values\nprint('Nan value',df_fr2.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.412959Z","iopub.execute_input":"2022-02-14T15:29:33.413234Z","iopub.status.idle":"2022-02-14T15:29:33.617952Z","shell.execute_reply.started":"2022-02-14T15:29:33.413177Z","shell.execute_reply":"2022-02-14T15:29:33.617002Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"###### Note : \nin this dataset we don't have as much features as in the previous one!","metadata":{}},{"cell_type":"markdown","source":"#### Arabic dataset","metadata":{}},{"cell_type":"markdown","source":"###### 1) Main dataset","metadata":{}},{"cell_type":"code","source":"df_ar.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.619193Z","iopub.execute_input":"2022-02-14T15:29:33.619410Z","iopub.status.idle":"2022-02-14T15:29:33.632242Z","shell.execute_reply.started":"2022-02-14T15:29:33.619382Z","shell.execute_reply":"2022-02-14T15:29:33.631617Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"df_ar.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.633913Z","iopub.execute_input":"2022-02-14T15:29:33.634247Z","iopub.status.idle":"2022-02-14T15:29:33.655800Z","shell.execute_reply.started":"2022-02-14T15:29:33.634190Z","shell.execute_reply":"2022-02-14T15:29:33.654869Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"print('Size of the dataset:')\nlen(df_ar)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.657371Z","iopub.execute_input":"2022-02-14T15:29:33.658017Z","iopub.status.idle":"2022-02-14T15:29:33.665228Z","shell.execute_reply.started":"2022-02-14T15:29:33.657972Z","shell.execute_reply":"2022-02-14T15:29:33.664298Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"#test if the data contains null values\nprint('Nan value',df_ar.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.666590Z","iopub.execute_input":"2022-02-14T15:29:33.667344Z","iopub.status.idle":"2022-02-14T15:29:33.680597Z","shell.execute_reply.started":"2022-02-14T15:29:33.667291Z","shell.execute_reply":"2022-02-14T15:29:33.679590Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"df_ar['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.682053Z","iopub.execute_input":"2022-02-14T15:29:33.682369Z","iopub.status.idle":"2022-02-14T15:29:33.698899Z","shell.execute_reply.started":"2022-02-14T15:29:33.682327Z","shell.execute_reply":"2022-02-14T15:29:33.697794Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"#### Class distribution \ncum = df_ar['target'].value_counts().to_frame()\ncum['HITId'] = cum.index\ncumfig, ax = plt.subplots(figsize=(5,5))\nsn.barplot(data=cum,x='HITId',y='target',ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.700374Z","iopub.execute_input":"2022-02-14T15:29:33.701306Z","iopub.status.idle":"2022-02-14T15:29:33.923622Z","shell.execute_reply.started":"2022-02-14T15:29:33.701247Z","shell.execute_reply":"2022-02-14T15:29:33.922686Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"###### 2) Assest dataset","metadata":{}},{"cell_type":"code","source":"df_ar2.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.924832Z","iopub.execute_input":"2022-02-14T15:29:33.925047Z","iopub.status.idle":"2022-02-14T15:29:33.936437Z","shell.execute_reply.started":"2022-02-14T15:29:33.925020Z","shell.execute_reply":"2022-02-14T15:29:33.935743Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"print('Size of the dataset:')\nlen(df_ar2)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.938031Z","iopub.execute_input":"2022-02-14T15:29:33.938352Z","iopub.status.idle":"2022-02-14T15:29:33.945526Z","shell.execute_reply.started":"2022-02-14T15:29:33.938321Z","shell.execute_reply":"2022-02-14T15:29:33.944912Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"#test if the data contains null values\nprint('Nan value',df_ar2.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.946444Z","iopub.execute_input":"2022-02-14T15:29:33.946834Z","iopub.status.idle":"2022-02-14T15:29:33.957491Z","shell.execute_reply.started":"2022-02-14T15:29:33.946798Z","shell.execute_reply":"2022-02-14T15:29:33.956856Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"markdown","source":"## Data processing","metadata":{}},{"cell_type":"markdown","source":"Since our task is a binary classification: negative or positive tweet, we will create a dataset that contains both negative and positive tweet. To do this, we will merge the previous dataset with the assest one that contains positive tweets. \n\nWhile merging, the assest doesn't contain the columns : HTDID, sentiment, directness, annotator_sentiment and group,some NaN values appeared. To avoid them these values will be replaced in each column with an appripriate value, for instance:\n\n\nsentiment-> 'normal', \n\ndirectness->'direct',\n\ngroupe-> 'nothing' and \n\nannotator_sentiment->'indifference'","metadata":{}},{"cell_type":"markdown","source":"#### French Dataset","metadata":{}},{"cell_type":"code","source":"# negative tweets contained\n# we put target = 0 negative\nneg = df_fr.loc[df_fr['target']!='normal','target'] = 0\nneg = df_fr\nneg.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.958661Z","iopub.execute_input":"2022-02-14T15:29:33.958999Z","iopub.status.idle":"2022-02-14T15:29:33.977214Z","shell.execute_reply.started":"2022-02-14T15:29:33.958970Z","shell.execute_reply":"2022-02-14T15:29:33.976605Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"#positive tweets contained in the assest dataset\npos = df_fr2.loc[df_fr2['label']==1]\n#since we have a several amount of data, we take a part of it\nn = len(pos)\npos= pos[0:int(n/100)]\nlen(pos)\npos= pos.rename(columns={'label':'target','text':'tweet'})","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:33.978319Z","iopub.execute_input":"2022-02-14T15:29:33.978700Z","iopub.status.idle":"2022-02-14T15:29:34.042048Z","shell.execute_reply.started":"2022-02-14T15:29:33.978658Z","shell.execute_reply":"2022-02-14T15:29:34.041224Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"# merging the two dataframes\ndata_fr = pd.concat([neg,pos], ignore_index=True, sort=False)\n#print(len(data_fr))\ndata_fr.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.043388Z","iopub.execute_input":"2022-02-14T15:29:34.043700Z","iopub.status.idle":"2022-02-14T15:29:34.067112Z","shell.execute_reply.started":"2022-02-14T15:29:34.043660Z","shell.execute_reply":"2022-02-14T15:29:34.066224Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"# fixing nan values\ndata_fr.loc[data_fr['sentiment'].isnull().values==True,'sentiment'] = 'normal'\ndata_fr.loc[data_fr['directness'].isnull().values==True,'directness'] = 'direct'\ndata_fr.loc[data_fr['group'].isnull().values==True,'group'] = 'nothing'\ndata_fr.loc[data_fr['annotator_sentiment'].isnull().values==True,'annotator_sentiment'] = 'indifference'\ndata_fr = sklearn.utils.shuffle(data_fr)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.072759Z","iopub.execute_input":"2022-02-14T15:29:34.073001Z","iopub.status.idle":"2022-02-14T15:29:34.091445Z","shell.execute_reply.started":"2022-02-14T15:29:34.072971Z","shell.execute_reply":"2022-02-14T15:29:34.090737Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"#data_fr = data_fr.drop(columns='HITId')\ndata_fr.reindex().head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.092854Z","iopub.execute_input":"2022-02-14T15:29:34.093074Z","iopub.status.idle":"2022-02-14T15:29:34.107586Z","shell.execute_reply.started":"2022-02-14T15:29:34.093046Z","shell.execute_reply":"2022-02-14T15:29:34.106868Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.pie(data_fr[\"target\"].value_counts(),labels=data_fr[\"target\"].value_counts().index,autopct=lambda p:f'{p:.2f}%',\n        shadow=True,colors=['mediumvioletred','darkturquoise'],labeldistance = 1.1,textprops={'fontsize': 14})\n\nplt.savefig(\"distribution des données dans les différentes classes.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.108665Z","iopub.execute_input":"2022-02-14T15:29:34.108996Z","iopub.status.idle":"2022-02-14T15:29:34.252825Z","shell.execute_reply.started":"2022-02-14T15:29:34.108966Z","shell.execute_reply":"2022-02-14T15:29:34.251616Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"def oversample(df):\n    classes = df.target.value_counts().to_dict()\n    most = max(classes.values())\n    classes_list = []\n    for key in classes:\n        classes_list.append(df[df['target'] == key]) \n    classes_sample = []\n    for i in range(1,len(classes_list)):\n        classes_sample.append(classes_list[i].sample(most, replace=True))\n    df_maybe = pd.concat(classes_sample)\n    final_df = pd.concat([df_maybe,classes_list[0]], axis=0)\n    final_df = final_df.reset_index(drop=True)\n    return final_df","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.254708Z","iopub.execute_input":"2022-02-14T15:29:34.255290Z","iopub.status.idle":"2022-02-14T15:29:34.267092Z","shell.execute_reply.started":"2022-02-14T15:29:34.255237Z","shell.execute_reply":"2022-02-14T15:29:34.266149Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"data_fr = oversample(data_fr)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.268951Z","iopub.execute_input":"2022-02-14T15:29:34.270250Z","iopub.status.idle":"2022-02-14T15:29:34.303872Z","shell.execute_reply.started":"2022-02-14T15:29:34.270193Z","shell.execute_reply":"2022-02-14T15:29:34.302846Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"# distribution of classes: 0,1\ncum = data_fr['target'].value_counts().to_frame()\ncum['tweet'] = cum.index\ncumfig, ax = plt.subplots(figsize=(5,5))\nsn.barplot(data=cum,x='tweet',y='target',ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.305839Z","iopub.execute_input":"2022-02-14T15:29:34.306502Z","iopub.status.idle":"2022-02-14T15:29:34.502225Z","shell.execute_reply.started":"2022-02-14T15:29:34.306452Z","shell.execute_reply":"2022-02-14T15:29:34.501384Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.pie(data_fr[\"target\"].value_counts(),labels=data_fr[\"target\"].value_counts().index,autopct=lambda p:f'{p:.2f}%',\n        shadow=True,colors=['mediumvioletred','darkturquoise'],labeldistance = 1.1,textprops={'fontsize': 14})\n\nplt.savefig(\"distribution des données dans les différentes classes.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:05:43.592896Z","iopub.execute_input":"2022-02-14T17:05:43.593271Z","iopub.status.idle":"2022-02-14T17:05:43.781336Z","shell.execute_reply.started":"2022-02-14T17:05:43.593231Z","shell.execute_reply":"2022-02-14T17:05:43.779645Z"},"trusted":true},"execution_count":262,"outputs":[]},{"cell_type":"markdown","source":"#### Arabic dataset","metadata":{}},{"cell_type":"code","source":"neg = df_ar.loc[df_ar['target']!='normal','target'] = 0\nneg = df_ar\nneg.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.504330Z","iopub.execute_input":"2022-02-14T15:29:34.504932Z","iopub.status.idle":"2022-02-14T15:29:34.522189Z","shell.execute_reply.started":"2022-02-14T15:29:34.504885Z","shell.execute_reply":"2022-02-14T15:29:34.521245Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"df_ar2.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.524122Z","iopub.execute_input":"2022-02-14T15:29:34.524800Z","iopub.status.idle":"2022-02-14T15:29:34.537030Z","shell.execute_reply.started":"2022-02-14T15:29:34.524753Z","shell.execute_reply":"2022-02-14T15:29:34.535855Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"#positive tweets contained in the assest dataset\ndf_ar2.loc[df_ar2['Sentiment']=='Positive']\ndf_ar2.loc[df_ar2['Sentiment']=='Positive','Sentiment'] = 1\npos = df_ar2.loc[df_ar2[\"Sentiment\"]==1]\npos= pos.rename(columns={'Sentiment':'target','Feed':'tweet'})\npos = pos.drop(columns={'ID'})","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.538308Z","iopub.execute_input":"2022-02-14T15:29:34.539006Z","iopub.status.idle":"2022-02-14T15:29:34.549631Z","shell.execute_reply.started":"2022-02-14T15:29:34.538967Z","shell.execute_reply":"2022-02-14T15:29:34.548983Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"pos.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.551071Z","iopub.execute_input":"2022-02-14T15:29:34.551495Z","iopub.status.idle":"2022-02-14T15:29:34.565387Z","shell.execute_reply.started":"2022-02-14T15:29:34.551463Z","shell.execute_reply":"2022-02-14T15:29:34.564278Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"# merging the two dataframes\ndata_ar = pd.concat([neg,pos], ignore_index=True, sort=False)\n#print(len(data_ar))\ndata_ar.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.567012Z","iopub.execute_input":"2022-02-14T15:29:34.567611Z","iopub.status.idle":"2022-02-14T15:29:34.591474Z","shell.execute_reply.started":"2022-02-14T15:29:34.567562Z","shell.execute_reply":"2022-02-14T15:29:34.590465Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"data_ar.loc[data_ar['sentiment'].isnull().values==True,'sentiment'] = 'normal'\ndata_ar.loc[data_ar['directness'].isnull().values==True,'directness'] = 'direct'\ndata_ar.loc[data_ar['group'].isnull().values==True,'group'] = 'nothing'\ndata_ar.loc[data_ar['annotator_sentiment'].isnull().values==True,'annotator_sentiment'] = 'indifference'\ndata_ar = sklearn.utils.shuffle(data_ar)\ndata_ar.drop(columns='HITId')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.592924Z","iopub.execute_input":"2022-02-14T15:29:34.593135Z","iopub.status.idle":"2022-02-14T15:29:34.619393Z","shell.execute_reply.started":"2022-02-14T15:29:34.593109Z","shell.execute_reply":"2022-02-14T15:29:34.618579Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.pie(data_ar[\"target\"].value_counts(),labels=data_ar[\"target\"].value_counts().index,autopct=lambda p:f'{p:.2f}%',\n        shadow=True,colors=['mediumvioletred','darkturquoise'],labeldistance = 1.1,textprops={'fontsize': 14})\n\nplt.savefig(\" ara distribution des données dans les différentes classes.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.620835Z","iopub.execute_input":"2022-02-14T15:29:34.621051Z","iopub.status.idle":"2022-02-14T15:29:34.773065Z","shell.execute_reply.started":"2022-02-14T15:29:34.621023Z","shell.execute_reply":"2022-02-14T15:29:34.772046Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"data_ar = oversample(data_ar)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.775259Z","iopub.execute_input":"2022-02-14T15:29:34.776027Z","iopub.status.idle":"2022-02-14T15:29:34.792127Z","shell.execute_reply.started":"2022-02-14T15:29:34.775972Z","shell.execute_reply":"2022-02-14T15:29:34.791088Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"# distribution of classes: 0,1\ncum = data_ar['target'].value_counts().to_frame()\ncum['tweet'] = cum.index\ncumfig, ax = plt.subplots(figsize=(5,5))\nsn.barplot(data=cum,x='tweet',y='target',ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:34.794313Z","iopub.execute_input":"2022-02-14T15:29:34.795074Z","iopub.status.idle":"2022-02-14T15:29:35.002716Z","shell.execute_reply.started":"2022-02-14T15:29:34.795019Z","shell.execute_reply":"2022-02-14T15:29:35.002123Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.pie(data_ar[\"target\"].value_counts(),labels=data_ar[\"target\"].value_counts().index,autopct=lambda p:f'{p:.2f}%',\n        shadow=True,colors=['mediumvioletred','darkturquoise'],labeldistance = 1.1,textprops={'fontsize': 14})\n\nplt.savefig(\" ara distribution des données dans les différentes classes.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:35.003968Z","iopub.execute_input":"2022-02-14T15:29:35.004359Z","iopub.status.idle":"2022-02-14T15:29:35.131258Z","shell.execute_reply.started":"2022-02-14T15:29:35.004329Z","shell.execute_reply":"2022-02-14T15:29:35.130247Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"def hash_fix(h):\n    h1 = re.sub(r'[0-9]+', '', h)\n    h2 = re.sub(r'#', '', h1)\n    h3 = segment(str(h2))\n    h4 = ' '.join(map(str, h3)) \n    return h4","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:29:35.133066Z","iopub.execute_input":"2022-02-14T15:29:35.133628Z","iopub.status.idle":"2022-02-14T15:29:35.142867Z","shell.execute_reply.started":"2022-02-14T15:29:35.133576Z","shell.execute_reply":"2022-02-14T15:29:35.141626Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"tok_ar = tk.WordTokenizer()\ntok_ar.train('/kaggle/input/twitter/ar_dataset.csv')\ndef prepro_ar(tweet):\n    arabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n                             َ    | # Fatha\n                             ً    | # Tanwin Fath\n                             ُ    | # Damma\n                             ٌ    | # Tanwin Damm\n                             ِ    | # Kasra\n                             ٍ    | # Tanwin Kasr\n                             ْ    | # Sukun\n                             ـ     # Tatwil/Kashida\n                         \"\"\", re.VERBOSE)\n    \n    user = \"*@user\" # l'etoile se met au début, car l'arabe se lit de droite vers la gauche\n    tweet = re.sub(arabic_diacritics, '', str(tweet))\n    tweet = re.sub('r'+user, \"\", tweet)\n    tweet = re.sub(r'(.)\\1+', \"\", tweet) \n    tweet = ar.strip_tashkeel(tweet)\n    tweet = ar.strip_tatweel(tweet)\n    tweet = tweet.replace(\"@\", \" \")\n    tweet = tweet.replace(\"_\", \" \")\n    tweet = re.sub(\"ى\", \"ي\", tweet)\n    tweet = re.sub(\"ؤ\", \"ء\", tweet)\n    tweet = re.sub(\"ئ\", \"ء\", tweet)\n    tweet = re.sub(\"ة\", \"ه\", tweet)\n    tweet = re.sub(\"گ\", \"ك\", tweet)\n    tweet = tweet.replace(\"آ\", \"ا\")\n    tweet = tweet.replace(\"إ\", \"ا\")\n    tweet = tweet.replace(\"أ\", \"ا\")\n    tweet = tweet.replace(\"ؤ\", \"و\")\n    tweet = tweet.replace(\"ئ\", \"ي\")\n    tweet = nltk.tokenize.word_tokenize(tweet)\n    tweet = [ISRIStemmer().suf32(w) for w in tweet]\n\n    return tweet","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:49:56.146503Z","iopub.execute_input":"2022-02-14T15:49:56.146856Z","iopub.status.idle":"2022-02-14T15:49:59.168433Z","shell.execute_reply.started":"2022-02-14T15:49:56.146820Z","shell.execute_reply":"2022-02-14T15:49:59.167555Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"code","source":"contractions = {\n        'administration':'admin',\n        'avec':'ac',\n        'beaucoup':'bp',\n        'c’est-à-dire':'cad',\n        'cependant':'cpd',\n        'chose':'ch',\n        'conclusion':'ccl',\n        'confer ':'cf',\n        'court terme':'ct',\n        'dans':'ds',\n        'dedans':'dd',\n        'définition':'déf',\n        'et cetera':'etc',\n        'être':'ê',\n        'exemple':'ex',\n        'extérieur':'ext',\n        'font':'ft',\n        'général':'gal',\n        'gouvernement':'gouv',\n        'grand':'gd',\n        'groupe':'gp',\n        'identique':'idel',\n        'introduction':'intro',\n        'jour':'jr',\n        'long terme':'lt',\n        'lorsque':'lsq',\n        'mais':'ms',\n        'même':'^m',\n        'moyen terme':'mt',\n        'nombre':'nb',\n        'nombreux':'nbx',\n        'nombre':'nb',\n        'nombreux':'nbx',\n        'observation':'obs',\n        'ordre du jour':'oj',\n        'page':'p',\n        'parce que':'pcq',\n        'pendant':'pdt',\n        'personne':'pers',\n        'point':'pt',\n        'peut-être':'pê',\n        'pour':'pr',\n        'pourtant':'prtt',\n        'quand':'qd',\n        'quantité':'qté',\n        'que':'q',\n        'quelqu’un':'qqn',\n        'quelque chose':'qqch',\n        'quelque':'qq',\n        'quelquefois':'qqf',\n        'question':'quest',\n        'rendez-vous':'rdv',\n        'responsabilité':'respité',\n        'seulement':'slt',\n        'solution':'sol',\n        'sont':'st',\n        'sous':'ss',\n        'souvent':'svt',\n        'temps':'tps',\n        'toujours':'tjrs',\n        'tous':'ts',\n        'tout':'tt',\n        'toute':'tte',\n        'toutes':'ttes',\n        'vous':'vs',\n        'le':'l\\'',\n        'me':'m\\'',\n        'de':'d\\'',\n        'te':'t\\'',\n        'se':'s\\'',\n        'ce':'c\\'',\n        'ne':'n\\'',\n        'que':'qu\\'',\n        'jusque':'jusqu\\'',\n        'lorsque':'lorsqu\\'',\n        'puisque':'puisqu\\'',\n        'quelque':'quelqu\\'',\n        'quoique':'quoiqu\\'',\n        'parce que':'parce qu\\'',\n        'tel que':'tel qu\\'',\n        'telle que':'telle qu\\'',       \n        'faculte':'fac',\n        'bien':'bin',    \n\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:50:19.811834Z","iopub.execute_input":"2022-02-14T15:50:19.812678Z","iopub.status.idle":"2022-02-14T15:50:19.827094Z","shell.execute_reply.started":"2022-02-14T15:50:19.812634Z","shell.execute_reply":"2022-02-14T15:50:19.825965Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"code","source":"french_stopwords = nltk.corpus.stopwords.words('french')\nlemmatizer = FrenchLefffLemmatizer()\nspell = SpellChecker(language='fr')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:50:20.598694Z","iopub.execute_input":"2022-02-14T15:50:20.599019Z","iopub.status.idle":"2022-02-14T15:50:23.056062Z","shell.execute_reply.started":"2022-02-14T15:50:20.598986Z","shell.execute_reply":"2022-02-14T15:50:23.055121Z"},"trusted":true},"execution_count":182,"outputs":[]},{"cell_type":"code","source":"def prepro_fr(tweet):\n    # prepare regex for char filtering\n    re_print = re.compile('[^%s]' % re.escape(string.printable))\n    # normalize unicode characters\n    tweet = normalize('NFD', tweet).encode('ascii','ignore')\n    tweet = tweet.decode('UTF-8')\n    #demojize\n    tweet = emoji.demojize(tweet)\n    if \"#\" in tweet:\n        tweet = hash_fix(tweet)\n    tweet = tweet.replace('user', '')\n    tweet = tweet.replace('@user', '')\n    tweet = tweet.replace('url', '')\n    tweet = re.sub( r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\",'',tweet)\n    # convert to lower case\n    tweet = tweet.lower()\n    tweet = tweet.replace('\\'', '\\' ') \n    # remove punctuation\n    tweet = re.sub(r\"\\p{P}\", lambda m: \"-\" if m.group(0) == \"-\" else \"\", tweet)\n    # tokenization\n    tweet = nltk.tokenize.word_tokenize(tweet)\n    #contractions\n    tweet = [list(contractions.keys())[list(contractions.values()).index(word)] if word in contractions.values() else word for word in tweet]\n    # stop words\n    tweet = [w for w in tweet if w not in french_stopwords]\n    # remove non-printable chars form each token\n    tweet = [re_print.sub('', w) for w in tweet]\n    # lemmatization\n    tweet = [lemmatizer.lemmatize(w) for w in tweet]\n    return tweet","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:50:23.058355Z","iopub.execute_input":"2022-02-14T15:50:23.058671Z","iopub.status.idle":"2022-02-14T15:50:23.070462Z","shell.execute_reply.started":"2022-02-14T15:50:23.058626Z","shell.execute_reply":"2022-02-14T15:50:23.069599Z"},"trusted":true},"execution_count":183,"outputs":[]},{"cell_type":"code","source":"data_ar.tweet = data_ar.tweet.apply(lambda t: prepro_ar(t))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:50:23.907277Z","iopub.execute_input":"2022-02-14T15:50:23.907605Z","iopub.status.idle":"2022-02-14T15:50:26.287164Z","shell.execute_reply.started":"2022-02-14T15:50:23.907570Z","shell.execute_reply":"2022-02-14T15:50:26.286301Z"},"trusted":true},"execution_count":184,"outputs":[]},{"cell_type":"code","source":"data_fr.tweet = data_fr.tweet.apply(lambda t: prepro_fr(t))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:50:42.664843Z","iopub.execute_input":"2022-02-14T15:50:42.665848Z","iopub.status.idle":"2022-02-14T15:51:37.227028Z","shell.execute_reply.started":"2022-02-14T15:50:42.665803Z","shell.execute_reply":"2022-02-14T15:51:37.225854Z"},"trusted":true},"execution_count":186,"outputs":[]},{"cell_type":"code","source":"!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ar.300.vec.gz","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:53:19.089274Z","iopub.execute_input":"2022-02-14T15:53:19.089589Z","iopub.status.idle":"2022-02-14T15:53:52.694024Z","shell.execute_reply.started":"2022-02-14T15:53:19.089530Z","shell.execute_reply":"2022-02-14T15:53:52.692824Z"},"trusted":true},"execution_count":188,"outputs":[]},{"cell_type":"code","source":"!gunzip cc.ar.300.vec.gz\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:53:52.696560Z","iopub.execute_input":"2022-02-14T15:53:52.696881Z","iopub.status.idle":"2022-02-14T15:54:54.461032Z","shell.execute_reply.started":"2022-02-14T15:53:52.696829Z","shell.execute_reply":"2022-02-14T15:54:54.459785Z"},"trusted":true},"execution_count":189,"outputs":[]},{"cell_type":"code","source":"!wc -l cc.ar.300.vec","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:54:54.462775Z","iopub.execute_input":"2022-02-14T15:54:54.463078Z","iopub.status.idle":"2022-02-14T15:54:56.437788Z","shell.execute_reply.started":"2022-02-14T15:54:54.463040Z","shell.execute_reply":"2022-02-14T15:54:56.436726Z"},"trusted":true},"execution_count":190,"outputs":[]},{"cell_type":"code","source":"f_ar = open('cc.ar.300.vec', encoding='utf-8')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:54:56.440683Z","iopub.execute_input":"2022-02-14T15:54:56.440958Z","iopub.status.idle":"2022-02-14T15:54:56.448830Z","shell.execute_reply.started":"2022-02-14T15:54:56.440925Z","shell.execute_reply":"2022-02-14T15:54:56.447447Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"code","source":"X_train_ar,X_test_ar, y_train_ar,y_test_ar = train_test_split(data_ar.tweet,data_ar.target.values,test_size=0.1)\n\n# text ---> integer sequence\nX_train_seq_ar = [tok_ar.encode_sentences(X_train_ar.values[i]) for i in range(len(X_train_ar))]\nX_test_seq_ar = [tok_ar.encode_sentences(X_test_ar.values[i]) for i in range(len(X_test_ar))]\n\n# integer sequences --> integer sequences with same length\nX_train_seq_ar = pad_sequences(X_train_seq_ar, maxlen=300)\nX_test_seq_ar = pad_sequences(X_test_seq_ar , maxlen=300)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:54:56.450299Z","iopub.execute_input":"2022-02-14T15:54:56.450579Z","iopub.status.idle":"2022-02-14T15:55:07.570582Z","shell.execute_reply.started":"2022-02-14T15:54:56.450529Z","shell.execute_reply":"2022-02-14T15:55:07.569626Z"},"trusted":true},"execution_count":192,"outputs":[]},{"cell_type":"code","source":"X_test_seq_ar","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:02:53.500414Z","iopub.execute_input":"2022-02-14T17:02:53.500790Z","iopub.status.idle":"2022-02-14T17:02:53.507922Z","shell.execute_reply.started":"2022-02-14T17:02:53.500754Z","shell.execute_reply":"2022-02-14T17:02:53.506976Z"},"trusted":true},"execution_count":260,"outputs":[]},{"cell_type":"code","source":"tweets_length_ar= [len(t) for t in X_train_ar.values]\nmax_seq_len_ar = max(tweets_length_ar)\nprint(max_seq_len_ar)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T16:05:53.120785Z","iopub.execute_input":"2022-02-14T16:05:53.121162Z","iopub.status.idle":"2022-02-14T16:05:53.131394Z","shell.execute_reply.started":"2022-02-14T16:05:53.121121Z","shell.execute_reply":"2022-02-14T16:05:53.130614Z"},"trusted":true},"execution_count":211,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preparing embedding  model!\n!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:56:54.223485Z","iopub.execute_input":"2022-02-14T15:56:54.223872Z","iopub.status.idle":"2022-02-14T15:57:28.628067Z","shell.execute_reply.started":"2022-02-14T15:56:54.223833Z","shell.execute_reply":"2022-02-14T15:57:28.626828Z"},"trusted":true},"execution_count":194,"outputs":[]},{"cell_type":"code","source":"!gunzip cc.fr.300.vec.gz","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:57:28.630405Z","iopub.execute_input":"2022-02-14T15:57:28.631216Z","iopub.status.idle":"2022-02-14T15:58:10.779331Z","shell.execute_reply.started":"2022-02-14T15:57:28.631125Z","shell.execute_reply":"2022-02-14T15:58:10.778171Z"},"trusted":true},"execution_count":195,"outputs":[]},{"cell_type":"code","source":"!wc -l cc.fr.300.vec","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:58:10.781113Z","iopub.execute_input":"2022-02-14T15:58:10.781378Z","iopub.status.idle":"2022-02-14T15:58:12.584840Z","shell.execute_reply.started":"2022-02-14T15:58:10.781346Z","shell.execute_reply":"2022-02-14T15:58:12.583581Z"},"trusted":true},"execution_count":196,"outputs":[]},{"cell_type":"code","source":"f_fr = open('cc.fr.300.vec', encoding='utf-8')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:58:12.587288Z","iopub.execute_input":"2022-02-14T15:58:12.587532Z","iopub.status.idle":"2022-02-14T15:58:12.592408Z","shell.execute_reply.started":"2022-02-14T15:58:12.587502Z","shell.execute_reply":"2022-02-14T15:58:12.591576Z"},"trusted":true},"execution_count":197,"outputs":[]},{"cell_type":"code","source":"#loading pretrained model to word vecs\nembeddings_index_fr = {}\nfor line in tqdm(f_fr):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    vector = np.asarray(values[1:],'float32')\n    embeddings_index_fr[word]=vector\nf_fr.close()\n\nprint('found %s word vectors' % len(embeddings_index_fr))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:58:12.593891Z","iopub.execute_input":"2022-02-14T15:58:12.594121Z","iopub.status.idle":"2022-02-14T16:01:24.605599Z","shell.execute_reply.started":"2022-02-14T15:58:12.594092Z","shell.execute_reply":"2022-02-14T16:01:24.604953Z"},"trusted":true},"execution_count":198,"outputs":[]},{"cell_type":"code","source":"labels = data_fr.target.values\n#splitting data into train and test set\nX_train_fr,X_test_fr, y_train_fr,y_test_fr = train_test_split(data_fr.tweet,labels,test_size=0.1)\n#counting the length of the tweet and taking the max\ntweets_length_fr= [len(t) for t in X_train_fr.values]\n\nmax_seq_len_fr = max(tweets_length_fr)\nprint(max_seq_len_fr)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T16:06:39.943107Z","iopub.execute_input":"2022-02-14T16:06:39.943660Z","iopub.status.idle":"2022-02-14T16:06:39.958319Z","shell.execute_reply.started":"2022-02-14T16:06:39.943598Z","shell.execute_reply":"2022-02-14T16:06:39.957602Z"},"trusted":true},"execution_count":212,"outputs":[]},{"cell_type":"code","source":"embed_dim = 300 ","metadata":{"execution":{"iopub.status.busy":"2022-02-14T16:06:42.001799Z","iopub.execute_input":"2022-02-14T16:06:42.002320Z","iopub.status.idle":"2022-02-14T16:06:42.008028Z","shell.execute_reply.started":"2022-02-14T16:06:42.002282Z","shell.execute_reply":"2022-02-14T16:06:42.007095Z"},"trusted":true},"execution_count":213,"outputs":[]},{"cell_type":"code","source":"tok_fr = Tokenizer()\ntok_fr.fit_on_texts(X_train_fr.values)\ntok_fr.fit_on_texts(X_test_fr.values)\nword_index_fr = tok_fr.word_index\nprint(\"dictionary size: \", len(word_index_fr))\n\n# text ---> integer sequence\nX_train_seq_fr = tok_fr.texts_to_sequences(X_train_fr.values) \nX_test_seq_fr = tok_fr.texts_to_sequences(X_test_fr.values)\n\n# integer sequences --> integer sequences with same length\nX_train_seq_fr = pad_sequences(X_train_seq_fr, maxlen=max_seq_len_fr)\nX_test_seq_fr = pad_sequences(X_test_seq_fr , maxlen=max_seq_len_fr)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T16:07:26.253343Z","iopub.execute_input":"2022-02-14T16:07:26.253720Z","iopub.status.idle":"2022-02-14T16:07:26.667600Z","shell.execute_reply.started":"2022-02-14T16:07:26.253687Z","shell.execute_reply":"2022-02-14T16:07:26.666654Z"},"trusted":true},"execution_count":217,"outputs":[]},{"cell_type":"code","source":"#embedding matrix\nprint('preparing embedding matrix...')\nwords_not_found_fr = []\nnb_words_fr = len(word_index_fr)\nembedding_matrix_fr = np.zeros((nb_words_fr, embed_dim))\nfor word, i in word_index_fr.items():\n    if i >= nb_words_fr:\n        continue\n    embedding_vector = embeddings_index_fr.get(word)\n    if (embedding_vector is not None) and len(embedding_vector) > 0:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix_fr[i] = embedding_vector\n    else:\n        words_not_found_fr.append(word)\nprint('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix_fr, axis=1) == 0))\nprint(len(embedding_matrix_fr))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T16:07:41.115062Z","iopub.execute_input":"2022-02-14T16:07:41.116301Z","iopub.status.idle":"2022-02-14T16:07:41.332635Z","shell.execute_reply.started":"2022-02-14T16:07:41.116250Z","shell.execute_reply":"2022-02-14T16:07:41.331480Z"},"trusted":true},"execution_count":220,"outputs":[]},{"cell_type":"code","source":"corrected = {}\nfor word in words_not_found_fr:\n        corrected[word] = spell.correction(word)\n    \nnot_found_fr = []\nembedding_matrix_fr = np.zeros((nb_words_fr, embed_dim))\nfor word, i in word_index_fr.items():\n    if word in words_not_found_fr:\n        word = corrected[word]\n    if i >= nb_words_fr:\n        continue\n    embedding_vector = embeddings_index_fr.get(word)\n    if (embedding_vector is not None) and len(embedding_vector) > 0:\n        embedding_matrix_fr[i] = embedding_vector\n    else:\n        not_found_fr.append(word)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:03:17.752094Z","iopub.execute_input":"2022-02-14T17:03:17.753032Z","iopub.status.idle":"2022-02-14T17:04:23.960081Z","shell.execute_reply.started":"2022-02-14T17:03:17.752991Z","shell.execute_reply":"2022-02-14T17:04:23.958847Z"},"trusted":true},"execution_count":261,"outputs":[]},{"cell_type":"code","source":"#not_found_fr","metadata":{"execution":{"iopub.status.busy":"2022-02-14T16:36:52.563676Z","iopub.status.idle":"2022-02-14T16:36:52.564728Z","shell.execute_reply.started":"2022-02-14T16:36:52.564519Z","shell.execute_reply":"2022-02-14T16:36:52.564553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_layer_fr = Embedding(\n                            nb_words_fr, \n                            embed_dim,  \n                            weights=[embedding_matrix_fr],\n                            input_length=max_seq_len_fr,\n                            trainable=False\n                    )","metadata":{"execution":{"iopub.status.busy":"2022-02-14T16:41:41.657044Z","iopub.execute_input":"2022-02-14T16:41:41.657936Z","iopub.status.idle":"2022-02-14T16:41:41.715569Z","shell.execute_reply.started":"2022-02-14T16:41:41.657886Z","shell.execute_reply":"2022-02-14T16:41:41.714877Z"},"trusted":true},"execution_count":223,"outputs":[]},{"cell_type":"code","source":"def NN(lang):\n    model = Sequential()\n    if lang=='AR':  \n        model.add(Embedding(tok_ar.vocab_size, max_seq_len_ar))\n    else:\n        model.add(embedding_layer_fr)\n    # 1st bi-LSTM layer\n    model.add(Bidirectional(LSTM(128,return_sequences=True)))\n    # 2nd bi-LSTM layer\n    model.add(Bidirectional(LSTM(64,return_sequences=False)))\n\n    #FC layers\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:07:26.840877Z","iopub.execute_input":"2022-02-14T17:07:26.841298Z","iopub.status.idle":"2022-02-14T17:07:26.851434Z","shell.execute_reply.started":"2022-02-14T17:07:26.841260Z","shell.execute_reply":"2022-02-14T17:07:26.850246Z"},"trusted":true},"execution_count":263,"outputs":[]},{"cell_type":"code","source":"early = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, mode='auto')\nnum_epochs = 10\nbatch_size = 128\n\nmodel_ar,model_fr = NN('AR'),NN('FR')\nprint('Arabic:')\nhistory_ar = model_ar.fit(X_train_seq_ar.astype('float32'), \n                            y_train_ar.astype('float32'), \n                            epochs=num_epochs,\n                            batch_size=batch_size,\n                            validation_split=0.15,\n                            callbacks = [early],\n                            verbose=1)\nprint('French:')\nhistory_fr = model_fr.fit(X_train_seq_fr.astype('float32'), \n                            y_train_fr.astype('float32'), \n                            epochs=num_epochs,\n                            batch_size=batch_size,\n                            validation_split=0.15,\n                            callbacks = [early],\n                            verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:07:28.500424Z","iopub.execute_input":"2022-02-14T17:07:28.500761Z","iopub.status.idle":"2022-02-14T17:21:15.983671Z","shell.execute_reply.started":"2022-02-14T17:07:28.500725Z","shell.execute_reply":"2022-02-14T17:21:15.982974Z"},"trusted":true},"execution_count":264,"outputs":[]},{"cell_type":"code","source":"def plot_acc_loss(history):\n    \"\"\"\n    Plot accuracy and loss of a model\n    @params:\n            - history: history of the model\n    @return:\n            plots\n    \"\"\"\n    fig,ax = plt.subplots(1,2,figsize=(10,5))\n    l = list(history.history.keys())\n    print(l)\n    # accuracy plot\n    ax[0].plot(history.history[l[1]])\n    ax[0].plot(history.history[l[3]])\n    ax[0].set_title('model accuracy')\n    ax[0].set_ylabel('accuracy')\n    ax[0].set_xlabel('epoch')\n    ax[0].legend(['train', 'test'], loc='upper left')\n    # loss plot\n    ax[1].plot(history.history[l[0]])\n    ax[1].plot(history.history[l[2]])\n    ax[1].set_title('model loss')\n    ax[1].set_ylabel('loss')\n    ax[1].set_xlabel('epoch')\n    ax[1].legend(['train', 'test'], loc='upper left')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:22:47.807865Z","iopub.execute_input":"2022-02-14T17:22:47.808246Z","iopub.status.idle":"2022-02-14T17:22:47.819612Z","shell.execute_reply.started":"2022-02-14T17:22:47.808210Z","shell.execute_reply":"2022-02-14T17:22:47.817976Z"},"trusted":true},"execution_count":267,"outputs":[]},{"cell_type":"code","source":"plot_acc_loss(history_ar)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:22:48.325866Z","iopub.execute_input":"2022-02-14T17:22:48.326391Z","iopub.status.idle":"2022-02-14T17:22:48.737789Z","shell.execute_reply.started":"2022-02-14T17:22:48.326347Z","shell.execute_reply":"2022-02-14T17:22:48.736399Z"},"trusted":true},"execution_count":268,"outputs":[]},{"cell_type":"code","source":"plot_acc_loss(history_fr)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:22:54.311202Z","iopub.execute_input":"2022-02-14T17:22:54.311504Z","iopub.status.idle":"2022-02-14T17:22:54.687956Z","shell.execute_reply.started":"2022-02-14T17:22:54.311473Z","shell.execute_reply":"2022-02-14T17:22:54.686827Z"},"trusted":true},"execution_count":269,"outputs":[]},{"cell_type":"code","source":"def predicted_label(model,x):\n    pred = model.predict(x)\n    print(pred)\n    lab_pred = []\n    for i in pred:\n        if i>0.5:\n            lab_pred.append(1)\n        else:\n            lab_pred.append(0)\n    return lab_pred","metadata":{"execution":{"iopub.status.busy":"2022-02-14T16:56:48.052935Z","iopub.execute_input":"2022-02-14T16:56:48.053224Z","iopub.status.idle":"2022-02-14T16:56:48.060427Z","shell.execute_reply.started":"2022-02-14T16:56:48.053195Z","shell.execute_reply":"2022-02-14T16:56:48.059309Z"},"trusted":true},"execution_count":229,"outputs":[]},{"cell_type":"code","source":"def plot_cm(model,x,y):\n    pred = model.predict(x)\n    y_pred = predicted_label(model,x)\n    cm = confusion_matrix(y,y_pred)  \n    sn.heatmap(cm, annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T16:56:49.293500Z","iopub.execute_input":"2022-02-14T16:56:49.294504Z","iopub.status.idle":"2022-02-14T16:56:49.299874Z","shell.execute_reply.started":"2022-02-14T16:56:49.294449Z","shell.execute_reply":"2022-02-14T16:56:49.298882Z"},"trusted":true},"execution_count":230,"outputs":[]},{"cell_type":"code","source":"print(\"Confusion Matrix for french dataset\")\n#plot_cm(model_fr,X_test_seq_fr,y_test_fr)\nprint(\"Confusion Matrix for arabic dataset\")\n#plot_cm(model_ar,X_test_seq_ar,y_test_ar)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:01:02.716885Z","iopub.execute_input":"2022-02-14T17:01:02.717253Z","iopub.status.idle":"2022-02-14T17:01:02.724824Z","shell.execute_reply.started":"2022-02-14T17:01:02.717213Z","shell.execute_reply":"2022-02-14T17:01:02.723660Z"},"trusted":true},"execution_count":255,"outputs":[]},{"cell_type":"code","source":"def classify_fr(tweet):\n    cleaned =  prepro_fr(tweet)\n    tok_fr.fit_on_texts(cleaned )\n    com_seq = tok_fr.texts_to_sequences(cleaned )\n    com_pad = pad_sequences(com_seq,maxlen=max_seq_len_fr, padding='post')\n    pred = model_fr.predict(com_pad)[0][0]\n    print(pred)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T16:58:58.617218Z","iopub.execute_input":"2022-02-14T16:58:58.617933Z","iopub.status.idle":"2022-02-14T16:58:58.622954Z","shell.execute_reply.started":"2022-02-14T16:58:58.617879Z","shell.execute_reply":"2022-02-14T16:58:58.622385Z"},"trusted":true},"execution_count":241,"outputs":[]},{"cell_type":"code","source":"classify_fr(\"salle arabe\")\nclassify_fr(\"il fait beau ici\")","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:23:03.772814Z","iopub.execute_input":"2022-02-14T17:23:03.773822Z","iopub.status.idle":"2022-02-14T17:23:05.665341Z","shell.execute_reply.started":"2022-02-14T17:23:03.773764Z","shell.execute_reply":"2022-02-14T17:23:05.664176Z"},"trusted":true},"execution_count":270,"outputs":[]},{"cell_type":"code","source":"def classify_ar(tweet):\n    cleaned =  prepro_ar(tweet)\n    com_seq= tok_ar.encode_sentences(cleaned, out_length = max_seq_len_ar)\n    com_pad = pad_sequences(com_seq,maxlen=max_seq_len_ar, padding='post')\n    pred = model_ar.predict(com_pad)[0][0]\n    print(pred)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T16:59:18.633750Z","iopub.execute_input":"2022-02-14T16:59:18.634052Z","iopub.status.idle":"2022-02-14T16:59:18.639871Z","shell.execute_reply.started":"2022-02-14T16:59:18.634018Z","shell.execute_reply":"2022-02-14T16:59:18.638947Z"},"trusted":true},"execution_count":248,"outputs":[]},{"cell_type":"code","source":"classify_ar(\"اانه جميل جدا\")","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:00:43.864908Z","iopub.execute_input":"2022-02-14T17:00:43.865256Z","iopub.status.idle":"2022-02-14T17:00:43.946208Z","shell.execute_reply.started":"2022-02-14T17:00:43.865223Z","shell.execute_reply":"2022-02-14T17:00:43.944792Z"},"trusted":true},"execution_count":253,"outputs":[]},{"cell_type":"code","source":"classify_ar(\"شعبك اغبياء\")","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:00:45.488485Z","iopub.execute_input":"2022-02-14T17:00:45.488833Z","iopub.status.idle":"2022-02-14T17:00:45.563798Z","shell.execute_reply.started":"2022-02-14T17:00:45.488799Z","shell.execute_reply":"2022-02-14T17:00:45.562794Z"},"trusted":true},"execution_count":254,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}