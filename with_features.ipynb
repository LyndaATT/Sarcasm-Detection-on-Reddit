{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"!pip install openpyxl\n!pip install PyArabic\n!pip install git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git &> /dev/null\n!pip install emoji \n!pip install Arabic-Stopwords\n!pip install tkseem\n!pip install tnkeeh","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-16T12:39:41.268759Z","iopub.execute_input":"2022-02-16T12:39:41.269175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install fr-word-segment","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyspellchecker","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\n\nimport nltk\nimport string\nfrom french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\nfrom fastai.text.all import *\n\nimport sklearn\nimport regex as re\nfrom unicodedata import normalize\n\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import AdamW\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import ToTensor\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import CyclicLR\nfrom torchvision import models\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport os\nimport gensim\n\n\n# keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import GRU,MaxPooling1D,GlobalMaxPooling1D,Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\nfrom keras import callbacks\nfrom keras.utils.vis_utils import plot_model\n\n# sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer \nfrom sklearn.metrics import roc_auc_score, accuracy_score,roc_curve, auc, plot_confusion_matrix, confusion_matrix\nfrom sklearn.svm import LinearSVC\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.manifold import TSNE\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.preprocessing.text import Tokenizer\nimport emoji\nfrom keras.models import Model\nimport seaborn as sn\nimport pyarabic.araby as ar\nimport tkseem as tk\nimport tnkeeh as tn\nfrom nltk.stem.isri import ISRIStemmer\nfrom spellchecker import SpellChecker\nfrom wordsegment import load,segment\nfrom keras.layers.merge import Concatenate\nimport tensorflow as tf\nload()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Data","metadata":{}},{"cell_type":"code","source":"print(tf. __version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ar = pd.read_csv('/kaggle/input/twitter/ar_dataset.csv')\ndf_ar2 = pd.read_excel('/kaggle/input/twitter/arr.xlsx')\n\n#main data\ndf_fr = pd.read_csv('/kaggle/input/twitter/fr_dataset.csv')\ndf_fr2 = pd.read_csv('/kaggle/input/twitter/french_tweets.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploring data","metadata":{}},{"cell_type":"markdown","source":"#### French Dataset","metadata":{}},{"cell_type":"markdown","source":"###### 1) Main dataset","metadata":{}},{"cell_type":"code","source":"df_fr.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_fr.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Size of the dataset:')\nlen(df_fr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test if the data contains null values\nprint('Nan value',df_fr.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#take a look at the column of the dataframe to see the features\ndf_fr.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_fr['sentiment'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Class distribution \ncum = df_fr['target'].value_counts().to_frame()\ncum['HITId'] = cum.index\ncumfig, ax = plt.subplots(figsize=(5,5))\nsn.barplot(data=cum,x='HITId',y='target',ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### 2) Assest dataset","metadata":{}},{"cell_type":"code","source":"df_fr2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Size of the dataset:')\nlen(df_fr2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test if the data contains null values\nprint('Nan value',df_fr2.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Note : \nin this dataset we don't have as much features as in the previous one!","metadata":{}},{"cell_type":"markdown","source":"#### Arabic dataset","metadata":{}},{"cell_type":"markdown","source":"###### 1) Main dataset","metadata":{}},{"cell_type":"code","source":"df_ar.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ar.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Size of the dataset:')\nlen(df_ar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test if the data contains null values\nprint('Nan value',df_ar.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ar['sentiment'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Class distribution \ncum = df_ar['target'].value_counts().to_frame()\ncum['HITId'] = cum.index\ncumfig, ax = plt.subplots(figsize=(5,5))\nsn.barplot(data=cum,x='HITId',y='target',ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### 2) Assest dataset","metadata":{}},{"cell_type":"code","source":"df_ar2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Size of the dataset:')\nlen(df_ar2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test if the data contains null values\nprint('Nan value',df_ar2.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data processing","metadata":{}},{"cell_type":"markdown","source":"Since our task is a binary classification: negative or positive tweet, we will create a dataset that contains both negative and positive tweet. To do this, we will merge the previous dataset with the assest one that contains positive tweets. \n\nWhile merging, the assest doesn't contain the columns : HTDID, sentiment, directness, annotator_sentiment and group,some NaN values appeared. To avoid them these values will be replaced in each column with an appripriate value, for instance:\n\n\nsentiment-> 'normal', \n\ndirectness->'direct',\n\ngroupe-> 'nothing' and \n\nannotator_sentiment->'indifference'","metadata":{}},{"cell_type":"markdown","source":"#### French Dataset","metadata":{}},{"cell_type":"code","source":"# negative tweets contained\n# we put target = 0 negative\nneg = df_fr.loc[df_fr['target']!='normal','target'] = 0\nneg = df_fr\nneg.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#positive tweets contained in the assest dataset\npos = df_fr2.loc[df_fr2['label']==1]\n#since we have a several amount of data, we take a part of it\nn = len(pos)\npos= pos[0:int(n/100)]\nlen(pos)\npos= pos.rename(columns={'label':'target','text':'tweet'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merging the two dataframes\ndata_fr = pd.concat([neg,pos], ignore_index=True, sort=False)\n#print(len(data_fr))\ndata_fr.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fixing nan values\ndata_fr.loc[data_fr['sentiment'].isnull().values==True,'sentiment'] = 'normal'\ndata_fr.loc[data_fr['directness'].isnull().values==True,'directness'] = 'direct'\ndata_fr.loc[data_fr['group'].isnull().values==True,'group'] = 'nothing'\ndata_fr.loc[data_fr['annotator_sentiment'].isnull().values==True,'annotator_sentiment'] = 'indifference'\ndata_fr = sklearn.utils.shuffle(data_fr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data_fr = data_fr.drop(columns='HITId')\ndata_fr.reindex().head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.pie(data_fr[\"target\"].value_counts(),labels=data_fr[\"target\"].value_counts().index,autopct=lambda p:f'{p:.2f}%',\n        shadow=True,colors=['mediumvioletred','darkturquoise'],labeldistance = 1.1,textprops={'fontsize': 14})\n\nplt.savefig(\"distribution des données dans les différentes classes.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def oversample(df):\n    classes = df.target.value_counts().to_dict()\n    most = max(classes.values())\n    classes_list = []\n    for key in classes:\n        classes_list.append(df[df['target'] == key]) \n    classes_sample = []\n    for i in range(1,len(classes_list)):\n        classes_sample.append(classes_list[i].sample(most, replace=True))\n    df_maybe = pd.concat(classes_sample)\n    final_df = pd.concat([df_maybe,classes_list[0]], axis=0)\n    final_df = final_df.reset_index(drop=True)\n    return final_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_fr = oversample(data_fr)\ndata_fr = data_fr.drop(columns={'HITId'})\ndata_fr.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distribution of classes: 0,1\ncum = data_fr['target'].value_counts().to_frame()\ncum['tweet'] = cum.index\ncumfig, ax = plt.subplots(figsize=(5,5))\nsn.barplot(data=cum,x='tweet',y='target',ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.pie(data_fr[\"target\"].value_counts(),labels=data_fr[\"target\"].value_counts().index,autopct=lambda p:f'{p:.2f}%',\n        shadow=True,colors=['mediumvioletred','darkturquoise'],labeldistance = 1.1,textprops={'fontsize': 14})\n\nplt.savefig(\"distribution des données dans les différentes classes.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Arabic dataset","metadata":{}},{"cell_type":"code","source":"neg = df_ar.loc[df_ar['target']!='normal','target'] = 0\nneg = df_ar\nneg.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ar2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#positive tweets contained in the assest dataset\ndf_ar2.loc[df_ar2['Sentiment']=='Positive']\ndf_ar2.loc[df_ar2['Sentiment']=='Positive','Sentiment'] = 1\npos = df_ar2.loc[df_ar2[\"Sentiment\"]==1]\npos= pos.rename(columns={'Sentiment':'target','Feed':'tweet'})\npos = pos.drop(columns={'ID'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merging the two dataframes\ndata_ar = pd.concat([neg,pos], ignore_index=True, sort=False)\n#print(len(data_ar))\ndata_ar.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_ar.loc[data_ar['sentiment'].isnull().values==True,'sentiment'] = 'normal'\ndata_ar.loc[data_ar['directness'].isnull().values==True,'directness'] = 'direct'\ndata_ar.loc[data_ar['group'].isnull().values==True,'group'] = 'nothing'\ndata_ar.loc[data_ar['annotator_sentiment'].isnull().values==True,'annotator_sentiment'] = 'indifference'\ndata_ar = sklearn.utils.shuffle(data_ar)\ndata_ar.drop(columns='HITId')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.pie(data_ar[\"target\"].value_counts(),labels=data_ar[\"target\"].value_counts().index,autopct=lambda p:f'{p:.2f}%',\n        shadow=True,colors=['mediumvioletred','darkturquoise'],labeldistance = 1.1,textprops={'fontsize': 14})\n\nplt.savefig(\" ara distribution des données dans les différentes classes.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_ar = oversample(data_ar)\ndata_ar = data_ar.drop(columns={'HITId'})\ndata_ar.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distribution of classes: 0,1\ncum = data_ar['target'].value_counts().to_frame()\ncum['tweet'] = cum.index\ncumfig, ax = plt.subplots(figsize=(5,5))\nsn.barplot(data=cum,x='tweet',y='target',ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.pie(data_ar[\"target\"].value_counts(),labels=data_ar[\"target\"].value_counts().index,autopct=lambda p:f'{p:.2f}%',\n        shadow=True,colors=['mediumvioletred','darkturquoise'],labeldistance = 1.1,textprops={'fontsize': 14})\n\nplt.savefig(\" ara distribution des données dans les différentes classes.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_fr.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#encoding labels\nle = preprocessing.LabelEncoder()\ndata_fr.sentiment = le.fit_transform(data_fr.sentiment)\ndata_fr.directness = le.fit_transform(data_fr.directness)\ndata_fr.annotator_sentiment = le.fit_transform(data_fr.annotator_sentiment)\ndata_fr.group = le.fit_transform(data_fr.group)\ndata_fr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_ar.sentiment = le.fit_transform(data_ar.sentiment)\ndata_ar.directness = le.fit_transform(data_ar.directness)\ndata_ar.annotator_sentiment = le.fit_transform(data_ar.annotator_sentiment)\ndata_ar.group = le.fit_transform(data_ar.group)\ndata_ar.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hash_fix(h):\n    h1 = re.sub(r'[0-9]+', '', h)\n    h2 = re.sub(r'#', '', h1)\n    h3 = segment(str(h2))\n    h4 = ' '.join(map(str, h3)) \n    return h4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok_ar = tk.WordTokenizer()\ntok_ar.train('/kaggle/input/twitter/ar_dataset.csv')\ndef prepro_ar(tweet):\n    arabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n                             َ    | # Fatha\n                             ً    | # Tanwin Fath\n                             ُ    | # Damma\n                             ٌ    | # Tanwin Damm\n                             ِ    | # Kasra\n                             ٍ    | # Tanwin Kasr\n                             ْ    | # Sukun\n                             ـ     # Tatwil/Kashida\n                         \"\"\", re.VERBOSE)\n    \n    user = \"*@user\" # l'etoile se met au début, car l'arabe se lit de droite vers la gauche\n    tweet = re.sub(arabic_diacritics, '', str(tweet))\n    tweet = re.sub('r'+user, \"\", tweet)\n    tweet = re.sub(r'(.)\\1+', \"\", tweet) \n    tweet = ar.strip_tashkeel(tweet)\n    tweet = ar.strip_tatweel(tweet)\n    tweet = tweet.replace(\"@\", \" \")\n    tweet = tweet.replace(\"_\", \" \")\n    tweet = re.sub(\"ى\", \"ي\", tweet)\n    tweet = re.sub(\"ؤ\", \"ء\", tweet)\n    tweet = re.sub(\"ئ\", \"ء\", tweet)\n    tweet = re.sub(\"ة\", \"ه\", tweet)\n    tweet = re.sub(\"گ\", \"ك\", tweet)\n    tweet = tweet.replace(\"آ\", \"ا\")\n    tweet = tweet.replace(\"إ\", \"ا\")\n    tweet = tweet.replace(\"أ\", \"ا\")\n    tweet = tweet.replace(\"ؤ\", \"و\")\n    tweet = tweet.replace(\"ئ\", \"ي\")\n    tweet = nltk.tokenize.word_tokenize(tweet)\n    tweet = [ISRIStemmer().suf32(w) for w in tweet]\n\n    return tweet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contractions = {\n        'administration':'admin',\n        'avec':'ac',\n        'beaucoup':'bp',\n        'c’est-à-dire':'cad',\n        'cependant':'cpd',\n        'chose':'ch',\n        'conclusion':'ccl',\n        'confer ':'cf',\n        'court terme':'ct',\n        'dans':'ds',\n        'dedans':'dd',\n        'définition':'déf',\n        'et cetera':'etc',\n        'être':'ê',\n        'exemple':'ex',\n        'extérieur':'ext',\n        'font':'ft',\n        'général':'gal',\n        'gouvernement':'gouv',\n        'grand':'gd',\n        'groupe':'gp',\n        'identique':'idel',\n        'introduction':'intro',\n        'jour':'jr',\n        'long terme':'lt',\n        'lorsque':'lsq',\n        'mais':'ms',\n        'même':'^m',\n        'moyen terme':'mt',\n        'nombre':'nb',\n        'nombreux':'nbx',\n        'nombre':'nb',\n        'nombreux':'nbx',\n        'observation':'obs',\n        'ordre du jour':'oj',\n        'page':'p',\n        'parce que':'pcq',\n        'pendant':'pdt',\n        'personne':'pers',\n        'point':'pt',\n        'peut-être':'pê',\n        'pour':'pr',\n        'pourtant':'prtt',\n        'quand':'qd',\n        'quantité':'qté',\n        'que':'q',\n        'quelqu’un':'qqn',\n        'quelque chose':'qqch',\n        'quelque':'qq',\n        'quelquefois':'qqf',\n        'question':'quest',\n        'rendez-vous':'rdv',\n        'responsabilité':'respité',\n        'seulement':'slt',\n        'solution':'sol',\n        'sont':'st',\n        'sous':'ss',\n        'souvent':'svt',\n        'temps':'tps',\n        'toujours':'tjrs',\n        'tous':'ts',\n        'tout':'tt',\n        'toute':'tte',\n        'toutes':'ttes',\n        'vous':'vs',\n        'le':'l\\'',\n        'me':'m\\'',\n        'de':'d\\'',\n        'te':'t\\'',\n        'se':'s\\'',\n        'ce':'c\\'',\n        'ne':'n\\'',\n        'que':'qu\\'',\n        'jusque':'jusqu\\'',\n        'lorsque':'lorsqu\\'',\n        'puisque':'puisqu\\'',\n        'quelque':'quelqu\\'',\n        'quoique':'quoiqu\\'',\n        'parce que':'parce qu\\'',\n        'tel que':'tel qu\\'',\n        'telle que':'telle qu\\'',       \n        'faculte':'fac',\n        'bien':'bin',    \n\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"french_stopwords = nltk.corpus.stopwords.words('french')\nlemmatizer = FrenchLefffLemmatizer()\nspell = SpellChecker(language='fr')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepro_fr(tweet):\n    # prepare regex for char filtering\n    re_print = re.compile('[^%s]' % re.escape(string.printable))\n    # normalize unicode characters\n    tweet = normalize('NFD', tweet).encode('ascii','ignore')\n    tweet = tweet.decode('UTF-8')\n    #demojize\n    tweet = emoji.demojize(tweet)\n    if \"#\" in tweet:\n        tweet = hash_fix(tweet)\n    tweet = tweet.replace('user', '')\n    tweet = tweet.replace('@user', '')\n    tweet = tweet.replace('url', '')\n    tweet = re.sub( r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\",'',tweet)\n    # convert to lower case\n    tweet = tweet.lower()\n    tweet = tweet.replace('\\'', '\\' ') \n    # remove punctuation\n    tweet = re.sub(r\"\\p{P}\", lambda m: \"-\" if m.group(0) == \"-\" else \"\", tweet)\n    # tokenization\n    tweet = nltk.tokenize.word_tokenize(tweet)\n    #contractions\n    tweet = [list(contractions.keys())[list(contractions.values()).index(word)] if word in contractions.values() else word for word in tweet]\n    # stop words\n    tweet = [w for w in tweet if w not in french_stopwords]\n    # remove non-printable chars form each token\n    tweet = [re_print.sub('', w) for w in tweet]\n    # lemmatization\n    tweet = [lemmatizer.lemmatize(w) for w in tweet]\n    return tweet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_ar.tweet = data_ar.tweet.apply(lambda t: prepro_ar(t))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_fr.tweet = data_fr.tweet.apply(lambda t: prepro_fr(t))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets = data_fr.tweet.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ar.300.vec.gz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gunzip cc.ar.300.vec.gz\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wc -l cc.ar.300.vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_ar = open('cc.ar.300.vec', encoding='utf-8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_fr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_ar.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_ar = data_ar.target.values\ndata_ar = data_ar.drop(columns={'target'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_ar,X_test_ar, y_train_ar,y_test_ar = train_test_split(data_ar,labels_ar,test_size=0.1)\n# text ---> integer sequence\nX_train_ar1 = X_train_ar.tweet.values\nX_test_ar1 = X_test_ar.tweet.values\n\nfor i in range(len(X_train_ar1)):\n    X_train_ar1[i] = tok_ar.encode_sentences(X_train_ar1[i]).flatten().astype(np.float32)\nfor i in range(len(X_test_ar1)):\n    X_test_ar1[i]  = tok_ar.encode_sentences(X_test_ar1[i]).flatten().astype(np.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_ar2 = X_train_ar[['sentiment', 'directness', 'annotator_sentiment', 'group']].values\nX_test_ar2 = X_test_ar[['sentiment', 'directness', 'annotator_sentiment', 'group']].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_length_ar= [len(X_train_ar1[i]) for i in range(len(X_train_ar1))]\nmax_seq_len_ar = max(tweets_length_ar)\nprint(max_seq_len_ar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preparing embedding  model!\n!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gunzip cc.fr.300.vec.gz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wc -l cc.fr.300.vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_fr = open('cc.fr.300.vec', encoding='utf-8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loading pretrained model to word vecs\nembeddings_index_fr = {}\nfor line in tqdm(f_fr):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    vector = np.asarray(values[1:],'float32')\n    embeddings_index_fr[word]=vector\nf_fr.close()\n\nprint('found %s word vectors' % len(embeddings_index_fr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_fr.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_fr = data_fr.target.values\ndata_fr = data_fr.drop(columns={'target'})\nX_train_fr,X_test_fr, y_train_fr,y_test_fr = train_test_split(data_fr,labels_fr,test_size=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_fr1 = X_train_fr.tweet.values\nX_test_fr1 = X_test_fr.tweet.values\n\ntok_fr = Tokenizer()\ntok_fr.fit_on_texts(data_fr.tweet.values)\n\nfor i in range(len(X_train_fr1)):\n    X_train_fr1[i] = tok_fr.texts_to_sequences(X_train_fr1[i])\n    X_train_fr1[i] = pad_sequences(X_train_fr1[i], maxlen=300)\nfor i in range(len(X_test_fr1)):\n    X_test_fr1[i] = tok_fr.texts_to_sequences(X_test_fr1[i])\n    X_test_fr1[i] = pad_sequences(X_test_fr1[i], maxlen=300)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_fr2 = X_train_fr[['sentiment', 'directness', 'annotator_sentiment', 'group']].values\nX_test_fr2 = X_test_fr[['sentiment', 'directness', 'annotator_sentiment', 'group']].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#counting the length of the tweet and taking the max\ntweets_length_fr= [len(X_train_fr1[i]) for i in range(len(X_train_fr1))]\nmax_seq_len_fr = max(tweets_length_fr)\nprint(max_seq_len_fr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 300 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#embedding matrix\nprint('preparing embedding matrix...')\nwords_not_found_fr = []\nword_index_fr = tok_fr.word_index\nnb_words_fr = len(word_index_fr)+1\nembedding_matrix_fr = np.zeros((nb_words_fr, embed_dim))\nfor word, i in word_index_fr.items():\n    if i >= nb_words_fr:\n        continue\n    embedding_vector = embeddings_index_fr.get(word)\n    if (embedding_vector is not None) and len(embedding_vector) > 0:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix_fr[i] = embedding_vector\n    else:\n        words_not_found_fr.append(word)\nprint('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix_fr, axis=1) == 0))\nprint(len(embedding_matrix_fr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_fr_1 = Input(shape=(max_seq_len_fr,))\ninput_ar_1 = Input(shape=(max_seq_len_ar,))\ninput_2 = Input(shape=(4,))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_layer_fr = Embedding(\n                            nb_words_fr, \n                            embed_dim,  \n                            weights=[embedding_matrix_fr],\n                            input_length=max_seq_len_fr,\n                            trainable=False\n                    )(input_fr_1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def NN(lang,input_1,input_2):\n    model = Sequential()\n    if lang=='AR':  \n        embedding_layer = Embedding(input_dim = tok_ar.vocab_size,output_dim = embed_dim, input_length=max_seq_len_ar)(input_ar_1)\n        #model.add(embedding_layer)\n    else:\n        embedding_layer = embedding_layer_fr\n        #model.add(embedding_layer_fr)\n    # 1st bi-LSTM layer\n    bi_lstm = Bidirectional(LSTM(128,return_sequences=True))(embedding_layer)\n    bilstm_dropout = Dropout(0.5)(bi_lstm)\n    x = GlobalMaxPool1D()(bilstm_dropout)\n    #FC layers\n    dense_layer_1 = Dense(10, activation='relu')(input_2)\n    #model.add(dense_layer_1)\n    dense_layer_2 = Dense(10, activation='relu')(dense_layer_1)\n    #model.add(Dense(64, activation='relu')(dense_layer_1))\n    #model.add(Dropout(0.5))\n    \n    concat_layer = Concatenate()([x, dense_layer_2])\n    dense_layer_3 = Dense(10, activation='relu')(concat_layer)\n    output = Dense(1, activation='sigmoid')(dense_layer_3)\n    #model.add(Dense(1, activation='sigmoid'))\n    model = Model(inputs=[input_1, input_2], outputs=output)\n    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tweets_ar = np.asarray([X_train_ar[0][0].astype('float32').flatten() for i in range(len(X_train_ar))])\n#tweets_fr = np.asarray([X_train_fr[0][0].astype('float32').flatten() for i in range(len(X_train_fr))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, mode='auto')\nnum_epochs = 10\nbatch_size = 128\n\nmodel_fr = NN('FR',input_fr_1,input_2)\nmodel_ar = NN('AR',input_ar_1,input_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train_ar1[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Arabic:')\nXar = [np.asarray(X_train_ar1), np.asarray(X_train_ar2)]\nhistory_ar = model_ar.fit(Xar,\n                          y_train_ar.astype(np.float32),\n                          batch_size=batch_size,\n                          epochs=num_epochs,\n                          validation_split=0.15,\n                          callbacks = [early],\n                          verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('French:')\nXfr = [X_train_fr1,X_train_fr2]\nhistory_fr = model_fr.fit(Xfr,\n                          y_train_fr.astype(np.float32),\n                          batch_size=batch_size,\n                          epochs=num_epochs,\n                          validation_split=0.15,\n                          callbacks = [early],\n                          verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_acc_loss(history):\n    \"\"\"\n    Plot accuracy and loss of a model\n    @params:\n            - history: history of the model\n    @return:\n            plots\n    \"\"\"\n    fig,ax = plt.subplots(1,2,figsize=(10,5))\n    l = list(history.history.keys())\n    print(l)\n    # accuracy plot\n    ax[0].plot(history.history[l[1]])\n    ax[0].plot(history.history[l[3]])\n    ax[0].set_title('model accuracy')\n    ax[0].set_ylabel('accuracy')\n    ax[0].set_xlabel('epoch')\n    ax[0].legend(['train', 'test'], loc='upper left')\n    # loss plot\n    ax[1].plot(history.history[l[0]])\n    ax[1].plot(history.history[l[2]])\n    ax[1].set_title('model loss')\n    ax[1].set_ylabel('loss')\n    ax[1].set_xlabel('epoch')\n    ax[1].legend(['train', 'test'], loc='upper left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_acc_loss(history_ar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_acc_loss(history_fr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predicted_label(model,x):\n    pred =np.array(model.predict(x))\n    lab_pred = []\n    for i in pred:\n        if i>=0.5:\n            lab_pred.append(1)\n        else:\n            lab_pred.append(0)\n    return lab_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_cm(model,x,y):\n    pred = model.predict(x)\n    y_pred = predicted_label(model,x)\n    cm = confusion_matrix(list(y),y_pred)  \n    sn.heatmap(cm, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Confusion Matrix for french dataset\")\nplot_cm(model_fr,X_test_seq_fr,y_test_fr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Confusion Matrix for arabic dataset\")\nplot_cm(model_ar,X_test_seq_ar,y_test_ar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def classify_fr(tweet):\n    cleaned =  prepro_fr(tweet)\n    tok_fr.fit_on_texts(cleaned )\n    com_seq = tok_fr.texts_to_sequences([cleaned])\n    com_pad = pad_sequences(com_seq,maxlen=max_seq_len_fr, padding='post')\n    pred = model_fr.predict(com_pad[0])\n    return pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classify_fr(\"gros batard\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def classify_ar(tweet):\n    cleaned =  prepro_ar(tweet)\n    com_seq= tok_ar.encode_sentences(cleaned)\n    com_pad = pad_sequences(com_seq,maxlen=max_seq_len_ar, padding='post')\n    pred = model_ar.predict(com_pad)[0][0]\n    print(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#classify_ar(\"اانه جميل جدا\")\ncleaned =  prepro_ar(\"اانه جميل جدا\")\ncom_seq= tok_ar.encode_sentences(cleaned, out_length = max_seq_len_ar)\ncom_pad = pad_sequences(com_seq,maxlen=max_seq_len_ar, padding='post')\npred = model_ar.predict(com_pad[0])[0]\ncom_pad ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classify_ar(\"شعبك اغبياء\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}